// Module included in the following assemblies:
//
// * nodes/nodes-nodes-resources-configuring.adoc

[id='nodes-nodes-resources-configuring-setting_{context}']
= Configuring allocated resources for nodes

{product-title} supports the CPU and memory resource types for allocation. If
your administrator enabled the ephemeral storage technology preview, the
`*ephemeral-resource*` resource type is supported as well. For the `*cpu*` type, the
resource quantity is specified in units of cores, such as `200m`, `0.5`, or `1`.
For `*memory*` and `*ephemeral-storage*`, it is specified in units of bytes,
such as `200Ki`, `50Mi`, or `5Gi`.

As an administrator, you can set these in the `*kubeletArguments*` section of the
node configuration map by using a set of `<resource_type>=<resource_quantity>` pairs 
(e.g., *cpu=200m,memory=512Mi*). Add the section if it does not already exist.

[NOTE]
====
Do not edit the `node-config.yaml` file directly.
==== 

.Procedure

. To help you determine setting for `*--system-reserved*` and `*--kube-reserved*` you can introspect the corresponding node's resource usage 
using the node summary API, which is accessible at *_<master>/api/v1/nodes/<node>/proxy/stats/summary_*. Run the following command for your node:
+
[source,bash]
----
curl <certificate details> https://<master>/api/v1/nodes/<node-name>/proxy/stats/summary
----
+
The _REST API Overview_ has details about certificate details.

+
For example, to access the resources from *cluster.node22* node, you can run:
+
[source,bash]
----
$ curl <certificate details> https://<master>/api/v1/nodes/cluster.node22/proxy/stats/summary
{
    "node": {
        "nodeName": "cluster.node22",
        "systemContainers": [
            {
                "cpu": {
                    "usageCoreNanoSeconds": 929684480915,
                    "usageNanoCores": 190998084
                },
                "memory": {
                    "rssBytes": 176726016,
                    "usageBytes": 1397895168,
                    "workingSetBytes": 1050509312
                },
                "name": "kubelet"
            },
            {
                "cpu": {
                    "usageCoreNanoSeconds": 128521955903,
                    "usageNanoCores": 5928600
                },
                "memory": {
                    "rssBytes": 35958784,
                    "usageBytes": 129671168,
                    "workingSetBytes": 102416384
                },
                "name": "runtime"
            }
        ]
    }
}
----

. Edit the `kubeletArguments` section of the node configuration map to set the allocatable resources as needed:
+
[source,yaml]
----
kubeletArguments:
  kube-reserved:
    - "cpu=200m,memory=512Mi"
  system-reserved:
    - "cpu=200m,memory=512Mi"
----

. Configure node enforcement as needed:
+
[source,yaml]
----
kubeletArguments:
  cgroups-per-qos:
    - "true" <1>
  cgroup-driver:
    - "systemd" <2>
  enforce-node-allocatable:
    - "pods" <3>
----
+
<1> Enable or disable the new cgroup hierarchy managed by the node.  Any change
of this setting requires a full drain of the node.  This flag must be true to allow the node to
enforce node allocatable.  We do not recommend users change this value.
<2> The cgroup driver used by the node when managing cgroup hierarchies.  This
value must match the driver associated with the container runtime.  Valid values
are `systemd` and `cgroupfs`.  The default is `systemd`.
<3> A comma-delimited list of scopes for where the node should enforce node
resource constraints.  Valid values are `pods`, `system-reserved`, and `kube-reserved`.
The default is `pods`.  We do not recommend users change this value.



// Module included in the following assemblies:
//
// * logging/efk-logging-elasticsearch.adoc

[id='efk-logging-elasticsearch-persistent-storage-local_{context}']
= Configuring an existing local file as storage for Elasticsearch

You can allocate a large file on an NFS server and mount the file to the nodes. You can then use the file as a host path device.

.Prerequisites

Allocate a large file on an NFS server and mount the file to the nodes

----
$ mount -F nfs nfserver:/nfs/storage/elasticsearch-1 /usr/local/es-storage
$ chown 1000:1000 /usr/local/es-storage
----

Then, use *_/usr/local/es-storage_* as a host-mount as described below.
Use a different backing file as storage for each Elasticsearch replica.

This loopback must be maintained manually outside of {product-title}, on the
node. You must not maintain it from inside a container.

.Procedure

. Assign the `hostpath` privilege to Elasticsearch:
+
----
$ oc adm policy add-scc-to-user privileged  system:serviceaccount:openshift-logging:elasticsearch
----

. Edit or create the Cluster Logging Custom Resource (CR):
+
[source,yaml]
----
spec
  managementState: Managed
  nodeSpec:
    image: brewregistry.stage.redhat.io/openshift/ose-logging-elasticsearch5:v4.0
    resources: {}
  nodes:
  - nodeCount: 1
    nodeSpec:
      resources: {}
    replicas: 1
    roles:
    - client
    - data
    - master
    storage:
      HostPath:
        path: /data
        type: DirectoryOrCreate
----

. Check that the deployment was created:
+
----
$ oc get deployment --selector component=elasticsearch -o jsonpath='{..spec.template.spec.volumes}'
----

. Check the index in *_/data_*:
+
----
#oc get pods --selector component=elasticsearch -o wide
ls /data in /data on node
----

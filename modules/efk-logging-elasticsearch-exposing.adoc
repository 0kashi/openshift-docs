// Module included in the following assemblies:
//
// * logging/efk-logging-elasticsearch.adoc

[id='efk-logging-elasticsearch-exposing_{context}']
= Exposing Elasticsearch as a route

By default, Elasticsearch deployed with cluster logging is not
accessible from outside the logging cluster. You can enable a re-encrypt route for external
access to Elasticsearch for those tools that want to access its data.

Internally, you can access Elasticsearch using your {product-title} token, and
you can provide the external Elasticsearch and Elasticsearch Ops
hostnames using the server certificate (similar to Kibana).

.Prerequisite

Set cluster logging to the unmanaged state.

.Procedure

. Use the following command to get name of the ElasticSearch pod:
+
----
ESPOD=$( oc get pods -l component=elasticsearch -o name | sed -e "s/pod\///" )
----

. Use the following command to extract CA certificate from Elasticsearch:
+
----
oc extract secret/elasticsearch --to=. --keys=admin-ca
----

. Create the re-encrypt route object for the ElasticSearch service as an yaml file:
+
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: elasticsearch
  namespace: openshift-logging
spec:
  host:
  to:
    kind: Service
    name: elasticsearch
  tls:
    termination: reencrypt <1>
    destinationCACertificate: |-   <2>
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
----
<1> Set the termination field to reencrypt.
<2> Add the contents of the *_admin-ca_* file.

. Use the following command to create the service:
+
----
oc create -f my_es_route.yaml
----

. Check that the ElasticSearch service is exposed:
+
----
curl --silent --insecure -H "Authorization: Bearer $( oc whoami -t )" "https://$( oc get route elasticsearch -o jsonpath='{.spec.host}' ):443/.operations.*/_search" | jq
----

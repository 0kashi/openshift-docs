// Module included in the following assemblies:
//
// * nodes/nodes-scheduler-descheduler.adoc

[id="nodes-scheduler-descheduling-policy-{context}"]
= Understanding descheduler policies

The Descheduler Operator creates a descheduler custom resource which you can use to specify the descheduling policy to implement.

There are four default policies that you can use.

Move pods to underutilized nodes::
The `lownodeutilization` strategy finds nodes that are underutilized and evicts pods from other nodes so that the evicted pods can be scheduled on these underutilized nodes.

.Sample `lownodeutilizaton` policy
[source,yaml]
----
spec:
  strategies:
    - name: lownodeutilization
      params:
       - name: cputhreshold <1>
         value: 10
       - name: memorythreshold
         value: 20
       - name: podsthreshold
         value: 30
       - name: memorytargetthreshold <2>
         value: 40
       - name: cputargetthreshold
         value: 50
       - name: podstargetthreshold
         value: 60
       - name: nodes
         value: 3  <3>
----

<1> Set the low-end thresholds for the `LowNodeUtilization` strategy. If the node is below all three values, the descheduler considers the node underutilized.
<2> Set the high-end thresholds for the `LowNodeUtilization` strategy. If the node is below these values and above the `threshold` values, the descheduler considers the node  properly utilized.
<3> Set the number of nodes that can be underutilized before the descheduler will evict pods from underutilized nodes.

The underutilization of nodes is determined by a configurable threshold, `thresholds`, for CPU, memory, or number of pods (based on percentage). If a node usage is below all these thresholds, the node is considered underutilized and the descheduler can evict pods from other nodes. Pods request resource requirements are considered when computing node resource utilization.

A high threshold value, `targetThresholds` is used to determine properly utilized nodes. Any node that is between the _thresholds_ and _targetThresholds_ is considered properly utilized and is not considered for eviction. The threshold, `targetThresholds`, can be configured for CPU, memory, and number of pods (based on percentage).

These thresholds could be tuned for your cluster requirements.

The `numberOfNodes` parameter can be configured to activate the strategy only when number of underutilized nodes is above the configured value. Set this parameter if it is acceptable for a few nodes to go underutilized. By default, `numberOfNodes` is set to zero.

Remove Duplicate Pods::
The `duplicates` strategy ensures that there is only one pod associated with a ReplicaSet, Replication Controller, DeploymentConfig, or Job running on same node.
If there are other pods associated with those objects, the duplicate pods are evicted. Removing duplicate pods results in better spreading of pods in a cluster.

.Sample `duplicates` policy
[source,yaml]
----
spec:
  strategies:
    - name: duplicates
      params: null
----

For example, duplicate pods could happen if a node fails and the pods on the node are moved to another node, leading to more than one pod associated with an ReplicaSet or Replication Controller, running on same node. After the failed node is ready again, this strategy could be used to evict those duplicate pods.

There are no parameters associated with this strategy.

Remove Pods Violating Inter-Pod Anti-Affinity::
The `interpodantiaffinity` strategy ensures that pods violating inter-pod anti-affinity are removed from nodes.

.Sample `interpodantiaffinity` policy
[source,yaml]
----
spec:
  strategies:
    - name: interpodantiaffinity
      params: null
----

For example, *Node1* has *podA*, *podB*, and *podC*. *podB* and *podC* have anti-affinity rules that prohibit them from running on the same node as *podA*. *podA* will be evicted from the node so that *podB* and *podC* can run on that node. This situation could happen if the anti-affinity rule was applied when *podB* and *podC* were running on the node.

Remove Pods Violating Node Affinity::
The `removepodsviolatingnodeaffinity` strategy ensures that pods violating node affinity are removed from nodes. 

.Sample `removepodsviolatingnodeaffinity` policy
[source,yaml]
----
spec:
  strategies:
    - name: removepodsviolatingnodeaffinity
      params: null
----

For example, *nodeA* has *podA* which satisfied the node affinity rules at the time of scheduling. If over time *nodeA* no longer satisfies the rule and *nodeB* is available that satisfies the node affinity rule, *podA* is be evicted from *nodeA*.
